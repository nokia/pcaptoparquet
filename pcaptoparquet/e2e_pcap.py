# Copyright 2025 Nokia
# Licensed under the BSD 3-Clause License.
# SPDX-License-Identifier: BSD-3-Clause

"""
This module provides the E2EPcap class for working with PCAP files.

The E2EPcap class represents a PCAP file and provides methods for exporting
the data in different formats.

Example usage:
    # Create an instance of E2EPcap
    pcap = E2EPcap(tags=["tag1", "tag2"], ctype="Client", pcap_full_name="example.pcap")

    # Export the data in raw format
    pcap.export(outformat="raw", output="output.txt")

    # Export the data in JSON format
    pcap.export(outformat="json", output="output.json")
"""
import datetime
import gc
import io
import json
import os
import struct
import sys
from typing import Any, Dict, Optional, Tuple

import dpkt
import polars as pl

from .e2e_config import E2EConfig
from .e2e_packet import E2EPacket
from .e2e_parallel import PCAPParallel


# Utility class to encode complex objects to JSON
class ComplexEncoder(json.JSONEncoder):
    """
    A JSON encoder that can handle complex objects.
    """

    def default(self, o: Any) -> Any:
        if hasattr(o, "to_json"):
            return o.to_json()

        return json.JSONEncoder.default(self, o)


# The E2EPcap class
class E2EPcap:
    """
    The E2EPcap class represents a PCAP file and provides
    methods for exporting the data in different formats.
    """

    # User provided tags and values
    _common_user_meta = {
        "collection_type": "category",
    }

    # PCAP file metadata
    _common_pcap_meta = {"encapsulation": "category", "snaplen": "UInt32"}

    @staticmethod
    def process_pcap_packet(
        num: int,
        timestamp: float,
        buf: bytes,
        encapsulation: str,
        transport_port_cb: dict[str, Any],
        meta_values: Optional[Dict[Any, Any]] = None,
    ) -> E2EPacket:
        """
        Process a single PCAP packet.
        """
        if meta_values is None:
            meta_values = {}

        utc_date_time = datetime.datetime.fromtimestamp(
            float(timestamp), datetime.timezone.utc
        )

        sll = None
        eth = None
        outerip = None

        try:
            if encapsulation == "Linux cooked capture":
                sll = dpkt.sll.SLL(buf)
                if isinstance(sll.data, dpkt.ethernet.Ethernet):
                    eth = sll.data
                if isinstance(sll.data, (dpkt.ip.IP, dpkt.ip6.IP6)):
                    outerip = sll.data
            elif encapsulation == "Ethernet":
                eth = dpkt.ethernet.Ethernet(buf)
            elif encapsulation == "Loopback Raw":
                if dpkt.compat_ord(buf[4]) == 0x45:
                    # IP version 4 + header len 20 bytes
                    outerip = dpkt.ip.IP(buf[4:])
                elif dpkt.compat_ord(buf[4]) & 0xF0 == 0x60:
                    # IP version 6
                    outerip = dpkt.ip6.IP6(buf[4:])
            elif encapsulation == "Raw IP":
                if dpkt.compat_ord(buf[0]) == 0x45:
                    # IP version 4 + header len 20 bytes
                    outerip = dpkt.ip.IP(buf)
                elif dpkt.compat_ord(buf[0]) & 0xF0 == 0x60:
                    # IP version 6
                    outerip = dpkt.ip6.IP6(buf)
        except Exception:  # pylint: disable=broad-except
            pass

        if outerip is None and eth is None:
            raise ValueError("Unknown encapsulation type: " + str(encapsulation))

        return E2EPacket(
            num,
            utc_date_time,
            eth,
            outerip,
            transport_port_cb,
            meta_values=meta_values,
        )

    @staticmethod
    def datalink_to_encapsulation(datalink: int) -> str:
        """
        Converts the datalink value to a string name.

        Args:
            datalink (int): The datalink value.

        Returns:
            str: The datalink name.
        """
        if datalink in [dpkt.pcap.DLT_LINUX_SLL]:
            encapsulation = "Linux cooked capture"
        elif datalink in [dpkt.pcap.DLT_EN10MB]:
            encapsulation = "Ethernet"
        elif datalink in [
            dpkt.pcap.DLT_NULL,
            dpkt.pcap.DLT_LOOP,
            dpkt.pcap.DLT_RAW,
        ]:
            encapsulation = "Loopback Raw"
        elif datalink in [101]:
            encapsulation = "Raw IP"
        else:
            raise ValueError("Unknown datalink type: " + str(datalink))

        return encapsulation

    @staticmethod
    def add_error_to_dict_list(
        pcap_dict_list: Dict[str, Any],
        pcap_dtypes: Dict[str, str],
        num: int,
        error_message: str,
    ) -> Dict[str, Any]:
        """
        Add error information to the dictionary list.
        This is used to add error information to the dictionary list
        when processing a PCAP file.
        """
        for key in pcap_dtypes:
            if key == "num":
                pcap_dict_list[key].append(num)
            elif key == "error":
                pcap_dict_list[key].append(True)
            elif key == "error_message":
                pcap_dict_list[key].append(error_message)
            else:
                pcap_dict_list[key].append(None)
        pcap_dict_list["not_decoded_data"].append(None)
        return pcap_dict_list

    @staticmethod
    def process_pcap_common(
        pcap: Any, params: Optional[dict[str, Any]] = None
    ) -> pl.DataFrame:
        """
        Process a partial PCAP file.
        This is a wrapper around process_pcap_packet that reads the PCAP file in chunks.
        """
        if params is None:
            pcap_dtypes = E2EPacket.get_dtypes()
            meta_values: dict[str, str] = {}
            encapsulation = E2EPcap.datalink_to_encapsulation(pcap.datalink())
            transport_port_cb = E2EConfig().get_transport_port_cb()
            outformat = "parquet"
            file_handle = None
            use_polars = True
            pktnum = 0
        else:
            pcap_dtypes = params["pcap_dtypes"]
            encapsulation = params["encapsulation"]
            transport_port_cb = params["transport_port_cb"]
            meta_values = params["meta_values"]
            outformat = params["outformat"]
            file_handle = params["file_handle"]
            use_polars = params["use_polars"]
            pktnum = params["pktnum"]

        if not use_polars and not file_handle:
            raise ValueError("File handle is required for non-polars output formats.")

        pcap_dict_list: dict[str, Any] = {}

        for key in pcap_dtypes:
            pcap_dict_list[key] = []

        pcap_dict_list["not_decoded_data"] = []

        num = 0
        sep_ = ""
        for timestamp, buf in pcap:

            num = num + 1

            if buf is None or len(buf) == 0:
                # Add empty values to the dictionary
                E2EPcap.add_error_to_dict_list(
                    pcap_dict_list, pcap_dtypes, num, "Empty buffer"
                )
                continue

            if pktnum in (0, num):
                try:
                    e2e_pkt = E2EPcap.process_pcap_packet(
                        num,
                        timestamp,
                        buf,
                        encapsulation,
                        transport_port_cb,
                        meta_values=meta_values,
                    )
                except (ValueError, struct.error, dpkt.UnpackError):
                    # Add empty values to the dictionary
                    E2EPcap.add_error_to_dict_list(
                        pcap_dict_list, pcap_dtypes, num, "Error processing packet"
                    )
                    continue

                if use_polars:
                    # Parquet or Polars
                    new_row = e2e_pkt.to_dict(pcap_dtypes)  # Convert to dict

                    for key in pcap_dtypes:
                        pcap_dict_list[key].append(new_row[key])

                    pcap_dict_list["not_decoded_data"].append(
                        e2e_pkt.get_not_decoded_data()
                    )

                else:
                    # Print the packet for non parquet formats
                    if outformat == "txt":
                        print(
                            str(e2e_pkt),
                            file=file_handle,
                        )
                    elif outformat == "json":
                        print(
                            sep_  # Ugly hack to avoid the first comma
                            + json.dumps(e2e_pkt.to_json(), cls=ComplexEncoder),
                            file=file_handle,
                        )
                        sep_ = ","

        if use_polars:
            # Convert dtypes based on pcap_dtypes
            pl_schema = E2EPcap.get_polars_schema(pcap_dtypes)

            pl_pcaparquet = pl.DataFrame(pcap_dict_list, schema=pl.Schema(pl_schema))

        else:
            pl_pcaparquet = pl.DataFrame()

            # Print the JSON header
            if outformat == "json":
                print("]}", file=file_handle)

        return pl_pcaparquet

    @staticmethod
    def process_partial_pcap(
        file_handle: Any, params: Optional[dict[str, Any]] = None
    ) -> io.BytesIO:
        """
        Process a partial PCAP file.
        This is a wrapper around process_pcap_packet that reads the PCAP file in chunks.
        """
        return io.BytesIO(
            E2EPcap.process_pcap_common(
                pcap=dpkt.pcap.Reader(file_handle), params=params
            ).serialize()
        )

    @staticmethod
    def process_partial_pcapng(
        file_handle: Any, params: Optional[dict[str, Any]] = None
    ) -> io.BytesIO:
        """
        Process a partial PCAP file.
        This is a wrapper around process_pcap_packet that reads the PCAP file in chunks.
        """
        return io.BytesIO(
            E2EPcap.process_pcap_common(
                pcap=dpkt.pcapng.Reader(file_handle), params=params
            ).serialize()
        )

    def add_tags_to_user_meta(self, tags: dict[str, str]) -> None:
        """
        Adds the user tags to the user metadata as categorical data.
        """
        for key in tags:
            new_key = E2EPacket.validate_str(key)
            self._common_user_meta[new_key] = "category"
            setattr(self, new_key, tags[key])

    def get_meta_values(
        self, meta_list: Optional[Tuple[Dict[str, str], Dict[str, str]]] = None
    ) -> Dict[str, str]:
        """
        Returns the metadata values as a list.

        Args:
            meta_list (tuple, optional): The list of metadata dictionaries.
            Defaults to an empty tuple.

        Returns:
            list: The metadata values as a list.
        """
        if meta_list is None:
            return {}
        meta_values = {}
        for meta in meta_list:
            for key in meta:
                meta_values[key] = getattr(self, key)
        return meta_values

    def to_json(self) -> dict[str, Any]:
        """
        Converts the E2EPcap object to a JSON-compatible dictionary.

        Args:
            d_ (dict, optional): The dictionary to populate with the object's
            attributes. Defaults to an empty dictionary.

        Returns:
            dict: The JSON-compatible dictionary.
        """
        d_ = {}
        for field_name in list(self._common_user_meta.keys()):
            d_[field_name] = getattr(self, field_name)
        for field_name in list(self._common_pcap_meta.keys()):
            d_[field_name] = getattr(self, field_name)
        d_["packets"] = []
        return d_

    def __init__(
        self,
        tags: Optional[dict[str, str]] = None,
        ctype: Optional[str] = None,
        pcap_full_name: Optional[str] = None,
        config: Optional[E2EConfig] = None,
        pcapng: bool = False,
        parallel: bool = False,
    ):
        """
        Initializes a new instance of the E2EPcap class.

        Args:
            tags (list): The user tags.
            ctype (str): The collection type.
            pcap_full_name (str): The full name of the PCAP file.
            config (E2EConfig): The E2EConfig object with protocol configuration
                and post-processing callbacks.
            pcapng (bool, optional): Whether the PCAP file is in PCAPNG format.
                Defaults to False.
            parallel (bool, optional): Whether to use parallel processing.
                Defaults to False.
            callbackpath (str, optional): The callback path. Defaults to None.
        """
        # * User Tags
        if tags is not None:
            self.add_tags_to_user_meta(tags)

        # * Collection Type (Unknown, Client, Network, Server)
        if ctype is not None and ctype in ["Client", "Network", "Server"]:
            self.collection_type = ctype
        else:
            self.collection_type = "Unknown"

        # Open PCAP
        self.pcap_name = str(pcap_full_name)
        if pcap_full_name is not None:
            pcapng = (
                pcapng
                or self.pcap_name.endswith(".pcapng")
                or ".pcapng." in self.pcap_name
            )
            # This handles compressed files...
            self.file = PCAPParallel.open_maybe_compressed(self.pcap_name)
        else:
            self.file = sys.stdin.buffer  # .raw

        if pcapng:
            self.pcap = dpkt.pcapng.Reader(self.file)
        else:
            self.pcap = dpkt.pcap.Reader(self.file)

        # Encapsulation
        self.datalink = self.pcap.datalink()
        self.encapsulation = self.datalink_to_encapsulation(self.datalink)

        # Snaplen
        self.snaplen = int(self.pcap.snaplen)

        # Parallel processing of the PCAP file
        # Only parallelize if the file is larger than a certain size.
        if pcap_full_name is not None:
            self.file_size = os.path.getsize(self.pcap_name)
            if not isinstance(self.file, io.BufferedReader):
                # Likely a compressed file assumme 4x the size
                self.file_size = self.file_size * 4
        else:
            # Standard input does not have a size
            self.file_size = 0

        self.ps = None
        if parallel and pcap_full_name and self.file_size > 500_000:  # 500KB
            # Only parallelize if the file is larger than
            # a certain size and the format is parquet.
            if pcapng:
                self.ps = PCAPParallel(
                    pcap_full_name,
                    callback=E2EPcap.process_partial_pcapng,
                )
            else:
                self.ps = PCAPParallel(
                    pcap_full_name,
                    callback=E2EPcap.process_partial_pcap,
                )

        if config is None:
            config = E2EConfig()

        # Protocol configurations
        self.transport_port_cb = config.get_transport_port_cb()

        # Post-processing Callbacks
        self.post_callbacks = config.get_post_callbacks()

    def __del__(self) -> None:
        """
        Closes the PCAP file when the object is destroyed.
        """
        if self.ps is None:
            self.file.close()

    @staticmethod
    def get_output_buffer(
        outformat: str = "parquet",
        output: Optional[str] = None,
        use_polars: bool = False,
    ) -> Any:
        """
        Returns the output buffer for the specified format.
        """
        f: Any = None

        if (outformat == "txt") or ((outformat == "json") and not use_polars):
            # Text output...
            if output:
                f = open(output, "w", encoding="utf-8")
            else:
                f = sys.stdout
        else:
            # Binary output...
            if output:
                f = open(output, "wb")
            else:
                f = sys.stdout.buffer
        return f

    @staticmethod
    def write_dataframe(
        df: pl.DataFrame, file_handle: Any, outformat: str, close_fh: bool
    ) -> None:
        """
        Writes the Polars DataFrame to the output file.
        """
        # Write to file
        if outformat == "parquet":
            df.write_parquet(file_handle)
        elif outformat == "txt":
            df.write_csv(
                file_handle, include_header=True, separator=E2EPacket.SEPARATOR
            )
        elif outformat == "json":
            df.write_ndjson(file_handle)

        file_handle.flush()

        if close_fh:
            file_handle.close()

    @staticmethod
    def get_polars_schema(pcap_dtypes: dict[str, str]) -> dict[str, Any]:
        """
        Returns the Polars schema for the specified PCAP data types.
        """
        pl_schema: dict[str, Any] = {}
        for key in pcap_dtypes:
            if pcap_dtypes[key] in ["UInt8"]:
                pl_schema[key] = pl.UInt8()
            elif pcap_dtypes[key] in ["UInt16"]:
                pl_schema[key] = pl.UInt16()
            elif pcap_dtypes[key] in ["UInt32"]:
                pl_schema[key] = pl.UInt32()
            elif pcap_dtypes[key] in ["UInt64"]:
                pl_schema[key] = pl.UInt64()
            elif pcap_dtypes[key] in ["Float32"]:
                pl_schema[key] = pl.Float32()
            elif pcap_dtypes[key] in ["Float64"]:
                pl_schema[key] = pl.Float64()
            elif pcap_dtypes[key] in ["boolean"]:
                pl_schema[key] = pl.Boolean()
            elif pcap_dtypes[key] in ["string"]:
                pl_schema[key] = pl.String()
            elif pcap_dtypes[key] in ["datetime64[ns, UTC]"]:
                pl_schema[key] = pl.Datetime(time_unit="ns", time_zone="UTC")
            elif pcap_dtypes[key] in ["category"]:
                pl_schema[key] = pl.Categorical()
            else:
                pl_schema[key] = pl.Object()

        pl_schema["not_decoded_data"] = pl.List(pl.UInt8())

        return pl_schema

    def export(
        self,
        outformat: str = "parquet",
        output: Optional[str] = None,
        pktnum: int = 0,
        return_df: bool = False,
    ) -> pl.DataFrame:
        """
        Exports the PCAP data in the specified format.

        Args:
            outformat (str, optional): The export format. Defaults to "txt".
            output (str, optional): The output file path. Defaults to None
                (prints to stdout).
            pktnum (int, optional): The packet number to export. Defaults to 0
                (exports all packets).

        Raises:
            ValueError: If an invalid format is specified.

        Example usage:
            # Export the data in txt format
            pcap.export(outformat="txt", output="output.txt")

            # Export the data in JSON format
            pcap.export(outformat="json", output="output.json")
        """
        use_polars = (
            self.ps is not None
            or len(self.post_callbacks) > 0
            or outformat == "parquet"
            or return_df
        )

        close_fh = False
        file_handle = None

        if not return_df:
            file_handle = E2EPcap.get_output_buffer(outformat, output, use_polars)
            close_fh = output is not None

        pcap_dtypes = E2EPacket.get_dtypes(
            (self._common_user_meta, self._common_pcap_meta)
        )

        if use_polars:
            # Create empty dictionary for the polars dataframe columns
            pcap_dict_list: dict[str, Any] = {}

            for key in pcap_dtypes:
                pcap_dict_list[key] = []

            pcap_dict_list["not_decoded_data"] = []

        elif outformat == "txt":
            # Print the CSV header
            print(
                E2EPacket.header((self._common_user_meta, self._common_pcap_meta)),
                file=file_handle,
            )
        elif outformat == "json":
            # Print the JSON header
            print(json.dumps(self.to_json(), cls=ComplexEncoder)[:-2], file=file_handle)

        meta_values = self.get_meta_values(
            (self._common_user_meta, self._common_pcap_meta)
        )

        # Multiprocessing
        if self.ps is not None:
            # Parallel processing
            partial_results = self.ps.split(
                params={
                    "encapsulation": self.encapsulation,
                    "transport_port_cb": self.transport_port_cb,
                    "meta_values": meta_values,
                    "pcap_dtypes": pcap_dtypes,
                    "outformat": outformat,
                    "file_handle": None,  # No file handle in parallel processing
                    "pktnum": pktnum,
                    "use_polars": use_polars,
                }
            )

            self.ps.shutdown()  # Shutdown the parallel processing

            pl_list: list[pl.DataFrame] = []

            # merge the results
            while len(partial_results) > 0:
                partial = partial_results.pop(0)
                # If the result is ready, add it to the list
                pl_list.append(pl.DataFrame.deserialize(partial.result()))

            del partial_results
            gc.collect()

            # Sort results by utc_date_time of the first packet
            pl_list.sort(key=lambda x: x["utc_date_time"][0])

            try:
                pl_pcaparquet = pl.concat(pl_list)
            except pl.exceptions.CategoricalRemappingWarning:
                # DataFrames are comming from different processes
                # and have different categories, so we need to remap them
                pass

            pl_list.clear()  # Clear the list to free memory
            del pl_list  # Clear the list to free memory
            gc.collect()

            total_len = len(pl_pcaparquet)

            # Create or update num column with sequential numbers
            # from 1 to total_len
            pl_pcaparquet = pl_pcaparquet.with_columns(
                pl.arange(1, total_len + 1).cast(pl.UInt32).alias("num")
            )

        else:
            # Single processing
            pl_pcaparquet = E2EPcap.process_pcap_common(
                pcap=self.pcap,
                params={
                    "encapsulation": self.encapsulation,
                    "transport_port_cb": self.transport_port_cb,
                    "meta_values": meta_values,
                    "pcap_dtypes": pcap_dtypes,
                    "outformat": outformat,
                    "file_handle": file_handle,
                    "pktnum": pktnum,
                    "use_polars": use_polars,
                },
            )

        if use_polars:
            # Call callback for further processing
            for callback in self.post_callbacks:
                pl_pcaparquet = callback(pl_pcaparquet)

            # Drop not_decoded_data column if exists
            if "not_decoded_data" in pl_pcaparquet.columns:
                pl_pcaparquet = pl_pcaparquet.drop("not_decoded_data")

            if return_df:
                return pl_pcaparquet

            E2EPcap.write_dataframe(pl_pcaparquet, file_handle, outformat, close_fh)

        if close_fh:
            file_handle.close()  # type: ignore
        else:
            sys.stdout.flush()

        return pl.DataFrame()
